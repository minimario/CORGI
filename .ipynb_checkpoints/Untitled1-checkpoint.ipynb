{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 48, 48, 8)         224       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 48, 48, 8)         584       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 48, 48, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 24, 24, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 24, 24, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 16)        1168      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 16)        2320      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 24, 24, 16)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               2359552   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 43)                11051     \n",
      "=================================================================\n",
      "Total params: 2,374,899\n",
      "Trainable params: 2,374,899\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.layers[-1].activation = None\n",
    "model.compile()\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 43)\n",
      "(43,)\n"
     ]
    }
   ],
   "source": [
    "# weights = model.layers[-1].get_weights()[0]\n",
    "# biases = model.layers[-1].get_weights()[1]\n",
    "print(weights.shape)\n",
    "print(biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "[-3. -4.  0.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "def fc(x_l, x_u, w1, b1, w2):\n",
    "    \"\"\"\n",
    "    [x: d neurons] -> [hidden: n neurons] -> [out: 1 neuron]\n",
    "    x_l: (d,)\n",
    "    x_u: (d,)\n",
    "    w1: (n, d)\n",
    "    b1: (n,)\n",
    "    w2: (1, n)\n",
    "    \"\"\"\n",
    "    d = x_l.shape[0]\n",
    "    n = w1.shape[0]\n",
    "    assert x_l.shape == x_u.shape == (d,)\n",
    "    assert w1.shape == (n, d)\n",
    "    assert b1.shape == (n,)\n",
    "    assert w2.shape == (1, n)\n",
    "    \n",
    "    w1_pos = np.maximum(w1, 0)\n",
    "    w1_neg = np.minimum(w1, 0)\n",
    "    layer1_min = w1_pos.dot(x_l) + w1_neg.dot(x_u) + b1\n",
    "    layer1_max = w1_pos.dot(x_u) + w1_neg.dot(x_l) + b1\n",
    "    \n",
    "    \n",
    "    grad_lo = np.zeros(x_l.shape)\n",
    "    grad_hi = np.zeros(x_l.shape)\n",
    "    \n",
    "    \n",
    "    # create diagonal array for unsure, positive, and negative neurons\n",
    "    unsure = np.logical_and(layer1_min <= 0, layer1_max >= 0)\n",
    "    positive = layer1_min >= 0\n",
    "    negative = layer1_max <= 0\n",
    "    \n",
    "    unsure_diag = np.diag(unsure).astype(np.float32)\n",
    "    pos_diag = np.diag(positive).astype(np.float32)\n",
    "    neg_diag = np.diag(negative).astype(np.float32)\n",
    "    \n",
    "    eps = 0.00001\n",
    "    assert(np.max(np.abs(unsure_diag + pos_diag + neg_diag - np.eye(n))) <= eps)\n",
    "\n",
    "    print(unsure_diag)\n",
    "    print(pos_diag)\n",
    "    print(neg_diag)\n",
    "    prod_matrix = np.diag(w2[0]).dot(w1)\n",
    "    prod_neg = np.minimum(0, prod_matrix)\n",
    "    prod_pos = np.maximum(0, prod_matrix)\n",
    "    for i in range(d):\n",
    "        for k in range(n):\n",
    "            assert(w1[k, i] * w2[0, k] == prod_matrix[k, i])\n",
    "            if unsure[k]:\n",
    "                grad_lo[i] += min(0, prod_matrix[k, i])\n",
    "                grad_hi[i] += max(0, prod_matrix[k, i])\n",
    "            elif layer1_min[k] >= 0:\n",
    "                grad_lo[i] += prod_matrix[k, i]\n",
    "                grad_hi[i] += prod_matrix[k, i]\n",
    "    print(grad_lo)\n",
    "    return grad_lo, grad_hi\n",
    "    \n",
    "\n",
    "# def f(x):\n",
    "#     return w2.dot(np.maximum(0, w1.dot(x) + b1))\n",
    "\n",
    "# def grad(x):\n",
    "#     x = x.astype(np.float32)\n",
    "#     ans = np.zeros(x.shape)\n",
    "#     for i in range(x.shape[0]):\n",
    "#         x2 = np.copy(x)\n",
    "#         eps = 0.01\n",
    "#         x2[i] += eps\n",
    "#         ans[i]= ((f(x2) - f(x)) / eps)\n",
    "#     return ans\n",
    "\n",
    "x_l = np.array([-1, 5, 3])\n",
    "x_u = np.array([1, 8, 6])\n",
    "w1 = np.random.randint(-3, 3, (4, 3))\n",
    "b1 = np.zeros(4,)\n",
    "w2 = np.ones((1, 4))\n",
    "# # print(x_l)\n",
    "# # print(x_u)\n",
    "# # print(w1)\n",
    "# # print(b1)\n",
    "# # print(w2)\n",
    "\n",
    "# x_real = np.array([-1, 3, 4])\n",
    "gl, gh = fc(x_l, x_u, w1, b1, w2)\n",
    "# # print(gl, gh)\n",
    "# # gl, gh = fc(x_real-0.01, x_real+0.01, w1, b1, w2)\n",
    "# def relu(x):\n",
    "#     return max(0, x)\n",
    "# def ff(a, b, c):\n",
    "#     return relu(-3*b) + relu(a-2*b+2*c) +relu(-3*a-2*b+2*c) +relu(-3*a-2*b+c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24, 24, 16)\n",
      "(1, 43)\n",
      "(9216,)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]]\n",
      "[-0.02441216 -0.03248248 -0.05113214 ...  0.02276646 -0.00961696\n",
      " -0.01227737]\n",
      "(9216,)\n"
     ]
    }
   ],
   "source": [
    "target_class = 13\n",
    "x = np.random.normal(0, 1, (1, 48, 48, 3))\n",
    "grad_model = tf.keras.models.Model(\n",
    "    [model.inputs], [model.get_layer(index = -4).output, model.output]\n",
    ")\n",
    "\n",
    "w1 = model.layers[-2].get_weights()[0]\n",
    "b1 = model.layers[-2].get_weights()[1]\n",
    "w2 = model.layers[-1].get_weights()[0][:, target_class:target_class+1]\n",
    "b2 = model.layers[-1].get_weights()[1][target_class]\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    inputs = tf.cast(x, tf.float64)\n",
    "    conv_outputs, predictions = grad_model(x)\n",
    "    print(conv_outputs.shape)\n",
    "    print(predictions.shape)\n",
    "    loss = predictions[:, target_class]\n",
    "\n",
    "grads = tape.gradient(loss, conv_outputs)\n",
    "grads_flat = grads.numpy().flatten()\n",
    "print(grads_flat.shape)\n",
    "\n",
    "x = conv_outputs.numpy().flatten()\n",
    "grad_lo, grad_hi = fc(x - 0.01, x + 0.01, w1.T, b1, w2.T)\n",
    "print(grad_lo.shape)\n",
    "eps = 0.00001\n",
    "assert(np.sum(np.logical_and(grad_lo - eps <= grads_flat, grads_flat <= grad_hi + eps)) == 9216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08210972662531085\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(grad_hi - grad_lo))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
